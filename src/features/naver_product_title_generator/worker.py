"""
ë„¤ì´ë²„ ìƒí’ˆëª… ìƒì„±ê¸° ì›Œì»¤
ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ ë° ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
"""
from typing import List, Dict, Any, Optional
from PySide6.QtCore import QThread, Signal, QMutex, QMutexLocker
import time

from src.foundation.logging import get_logger
from src.toolbox.progress import calc_percentage

from .models import (
    AnalysisStep, KeywordBasicData, ProductNameData, AIAnalysisResult, GeneratedTitle
)
from .adapters import parse_keywords, collect_product_names_for_keywords

logger = get_logger("features.naver_product_title_generator.worker")


class BasicAnalysisWorker(QThread):
    """2ë‹¨ê³„: ê¸°ì´ˆë¶„ì„ ì›Œì»¤"""
    
    # ì‹œê·¸ë„ ì •ì˜
    progress_updated = Signal(int, str)  # progress%, message
    analysis_completed = Signal(list)    # List[KeywordBasicData]
    error_occurred = Signal(str)         # error_message
    
    def __init__(self, product_name: str):
        super().__init__()
        self.product_name = product_name
        self._stop_requested = False
        self._mutex = QMutex()
    
    def request_stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­"""
        with QMutexLocker(self._mutex):
            self._stop_requested = True
            
    def stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ (í•˜ìœ„ í˜¸í™˜)"""
        self.request_stop()
    
    def is_stopped(self) -> bool:
        """ì¤‘ë‹¨ ìš”ì²­ í™•ì¸"""
        with QMutexLocker(self._mutex):
            return self._stop_requested
    
    def run(self):
        """ì›Œì»¤ ì‹¤í–‰"""
        try:
            logger.info(f"ê¸°ì´ˆë¶„ì„ ì‹œì‘: {self.product_name}")
            
            # 1ë‹¨ê³„: í‚¤ì›Œë“œ íŒŒì‹±
            self.progress_updated.emit(0, "í‚¤ì›Œë“œ íŒŒì‹± ì¤‘...")
            
            if self.is_stopped():
                return
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = parse_keywords(self.product_name)
            
            if not keywords:
                self.error_occurred.emit("ë¶„ì„í•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(20, f"{len(keywords)}ê°œ í‚¤ì›Œë“œ íŒŒì‹± ì™„ë£Œ")
            
            # 2ë‹¨ê³„: í‚¤ì›Œë“œë³„ ì›”ê²€ìƒ‰ëŸ‰ ë° ì¹´í…Œê³ ë¦¬ ë¶„ì„
            self.progress_updated.emit(30, "ë„¤ì´ë²„ API ë¶„ì„ ì¤‘...")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ë° ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from .models import KeywordBasicData
            
            analyzed_keywords = []
            
            # 1ë‹¨ê³„: ì›”ê²€ìƒ‰ëŸ‰ + ì¹´í…Œê³ ë¦¬ + ì „ì²´ìƒí’ˆìˆ˜ ëª¨ë‘ ì¡°íšŒ
            from .adapters import analyze_keywords_with_volume_and_category
            
            # ì§„í–‰ë¥  ì½œë°± ì •ì˜
            def progress_callback(current: int, total: int, message: str):
                if self.is_stopped():
                    return
                    
                progress = 30 + int((current / total) * 60)  # 30% ~ 90%
                self.progress_updated.emit(progress, f"í‚¤ì›Œë“œ ë¶„ì„ {current}/{total}: {message}")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
            analyzed_keywords = analyze_keywords_with_volume_and_category(
                keywords=keywords,
                max_workers=3,  # 3ê°œì”© ë³‘ë ¬ ì²˜ë¦¬
                stop_check=self.is_stopped,
                progress_callback=progress_callback
            )
            
            # ê²°ê³¼ ì •ë ¬ (ì›ë˜ ìˆœì„œ ìœ ì§€)
            keyword_order = {kw: i for i, kw in enumerate(keywords)}
            analyzed_keywords.sort(key=lambda x: keyword_order.get(x.keyword, 999999))
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(90, "í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ")
            
            # ê²€ìƒ‰ëŸ‰ì´ 0ë³´ë‹¤ í° í‚¤ì›Œë“œë§Œ í•„í„°ë§
            valid_keywords = [kw for kw in analyzed_keywords if kw.search_volume > 0]
            
            if not valid_keywords:
                # ê²€ìƒ‰ëŸ‰ì´ ì—†ì–´ë„ ëª¨ë“  í‚¤ì›Œë“œ ë°˜í™˜ (ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆë„ë¡)
                valid_keywords = analyzed_keywords
            
            self.progress_updated.emit(100, f"ë¶„ì„ ì™„ë£Œ: {len(valid_keywords)}ê°œ í‚¤ì›Œë“œ")
            
            # ì™„ë£Œ ì‹œê·¸ë„ ë°œì†¡
            self.analysis_completed.emit(valid_keywords)
            
            logger.info(f"ê¸°ì´ˆë¶„ì„ ì™„ë£Œ: {len(valid_keywords)}ê°œ í‚¤ì›Œë“œ")
            
        except Exception as e:
            logger.error(f"ê¸°ì´ˆë¶„ì„ ì‹¤íŒ¨: {e}")
            self.error_occurred.emit(f"ê¸°ì´ˆë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


class ProductNameCollectionWorker(QThread):
    """2ë‹¨ê³„: ìƒí’ˆëª… ìˆ˜ì§‘ ì›Œì»¤"""
    
    # ì‹œê·¸ë„ ì •ì˜
    progress_updated = Signal(int, str)  # progress%, message
    collection_completed = Signal(list)  # List[Dict] - ìƒí’ˆëª… ë°ì´í„°
    error_occurred = Signal(str)         # error_message
    
    def __init__(self, selected_keywords: List[KeywordBasicData]):
        super().__init__()
        self.selected_keywords = selected_keywords
        self._stop_requested = False
        self._mutex = QMutex()
    
    def request_stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­"""
        with QMutexLocker(self._mutex):
            self._stop_requested = True
            
    def stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ (í•˜ìœ„ í˜¸í™˜)"""
        self.request_stop()
    
    def is_stopped(self) -> bool:
        """ì¤‘ë‹¨ ìš”ì²­ í™•ì¸"""
        with QMutexLocker(self._mutex):
            return self._stop_requested
    
    def run(self):
        """ì›Œì»¤ ì‹¤í–‰"""
        try:
            logger.info(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì‹œì‘: {len(self.selected_keywords)}ê°œ í‚¤ì›Œë“œ")
            
            if not self.selected_keywords:
                self.error_occurred.emit("ì„ íƒëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # í‚¤ì›Œë“œ ë¬¸ìì—´ ì¶”ì¶œ
            keywords = [kw.keyword for kw in self.selected_keywords]
            
            self.progress_updated.emit(10, f"{len(keywords)}ê°œ í‚¤ì›Œë“œë¡œ ìƒí’ˆëª… ìˆ˜ì§‘ ì‹œì‘...")
            
            if self.is_stopped():
                return
            
            # ê° í‚¤ì›Œë“œë³„ë¡œ ìƒí’ˆëª… ìˆ˜ì§‘ (ì§„í–‰ë¥  ì—…ë°ì´íŠ¸)
            total_keywords = len(keywords)
            collected_data = []
            
            for i, keyword in enumerate(keywords):
                if self.is_stopped():
                    return
                
                progress = 20 + int((i / total_keywords) * 60)  # 20~80%
                self.progress_updated.emit(progress, f"ìƒí’ˆëª… ìˆ˜ì§‘ ì¤‘... ({i+1}/{total_keywords}) '{keyword}'")
                
                # í‚¤ì›Œë“œë³„ ìƒí’ˆëª… ìˆ˜ì§‘
                try:
                    keyword_products = collect_product_names_for_keywords([keyword], 40)
                    collected_data.extend(keyword_products)
                    
                    # ì§§ì€ ëŒ€ê¸° (API ê³¼ë¶€í•˜ ë°©ì§€)
                    time.sleep(0.2)
                    
                except Exception as e:
                    logger.warning(f"í‚¤ì›Œë“œ {keyword} ìƒí’ˆëª… ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    continue
                
                if self.is_stopped():
                    return
            
            self.progress_updated.emit(85, "ì¤‘ë³µ ì œê±° ì¤‘...")
            
            if self.is_stopped():
                return
            
            # ì „ì²´ ì¤‘ë³µ ì œê±°
            final_products = collect_product_names_for_keywords(keywords, 40)
            
            self.progress_updated.emit(100, f"ìƒí’ˆëª… ìˆ˜ì§‘ ì™„ë£Œ: {len(final_products)}ê°œ")
            
            # ì™„ë£Œ ì‹œê·¸ë„ ë°œì†¡
            self.collection_completed.emit(final_products)
            
            logger.info(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì™„ë£Œ: {len(final_products)}ê°œ")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.error_occurred.emit(f"ìƒí’ˆëª… ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")



class WorkerManager:
    """ì›Œì»¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.current_worker: Optional[QThread] = None
        self.worker_history = []
    
    def start_worker(self, worker: QThread) -> bool:
        """ìƒˆ ì›Œì»¤ ì‹œì‘"""
        try:
            # ê¸°ì¡´ ì›Œì»¤ê°€ ìˆìœ¼ë©´ ì •ë¦¬
            self.stop_current_worker()
            
            # ìƒˆ ì›Œì»¤ ì‹œì‘
            self.current_worker = worker
            self.worker_history.append(worker)
            worker.start()
            
            logger.info(f"ì›Œì»¤ ì‹œì‘: {worker.__class__.__name__}")
            return True
            
        except Exception as e:
            logger.error(f"ì›Œì»¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False
    
    def stop_current_worker(self) -> bool:
        """í˜„ì¬ ì›Œì»¤ ì¤‘ë‹¨"""
        if self.current_worker and self.current_worker.isRunning():
            try:
                # ì›Œì»¤ì— ì¤‘ë‹¨ ìš”ì²­
                if hasattr(self.current_worker, 'stop'):
                    self.current_worker.stop()
                
                # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
                if not self.current_worker.wait(5000):
                    logger.warning("ì›Œì»¤ê°€ 5ì´ˆ ë‚´ì— ì¢…ë£Œë˜ì§€ ì•ŠìŒ, ê°•ì œ ì¢…ë£Œ")
                    self.current_worker.terminate()
                    self.current_worker.wait(2000)
                
                logger.info(f"ì›Œì»¤ ì¤‘ë‹¨ ì™„ë£Œ: {self.current_worker.__class__.__name__}")
                return True
                
            except Exception as e:
                logger.error(f"ì›Œì»¤ ì¤‘ë‹¨ ì‹¤íŒ¨: {e}")
                return False
        
        return True
        
    def stop_worker(self, worker: QThread) -> bool:
        """íŠ¹ì • ì›Œì»¤ ì¤‘ë‹¨"""
        if worker and worker.isRunning():
            try:
                # ì›Œì»¤ì— ì¤‘ë‹¨ ìš”ì²­
                if hasattr(worker, 'request_stop'):
                    worker.request_stop()
                elif hasattr(worker, 'stop'):
                    worker.stop()
                
                # ìµœëŒ€ 3ì´ˆ ëŒ€ê¸°
                if not worker.wait(3000):
                    logger.warning(f"ì›Œì»¤ê°€ 3ì´ˆ ë‚´ì— ì¢…ë£Œë˜ì§€ ì•ŠìŒ: {worker.__class__.__name__}")
                    worker.terminate()
                    worker.wait(1000)
                
                logger.info(f"ì›Œì»¤ ì¤‘ë‹¨ ì™„ë£Œ: {worker.__class__.__name__}")
                return True
                
            except Exception as e:
                logger.error(f"ì›Œì»¤ ì¤‘ë‹¨ ì‹¤íŒ¨: {e}")
                return False
        
        return True
    
    def cleanup_all_workers(self):
        """ëª¨ë“  ì›Œì»¤ ì •ë¦¬"""
        self.stop_current_worker()
        
        # íˆìŠ¤í† ë¦¬ì˜ ëª¨ë“  ì›Œì»¤ë“¤ë„ ì •ë¦¬
        for worker in self.worker_history:
            if worker.isRunning():
                try:
                    if hasattr(worker, 'stop'):
                        worker.stop()
                    worker.wait(2000)
                except:
                    pass
        
        self.worker_history.clear()
        self.current_worker = None
        
        logger.info("ëª¨ë“  ì›Œì»¤ ì •ë¦¬ ì™„ë£Œ")
    
    def is_working(self) -> bool:
        """í˜„ì¬ ì‘ì—… ì¤‘ì¸ì§€ í™•ì¸"""
        return self.current_worker is not None and self.current_worker.isRunning()


class AIAnalysisWorker(QThread):
    """3ë‹¨ê³„: AI í‚¤ì›Œë“œ ë¶„ì„ ì›Œì»¤"""
    
    # ì‹œê·¸ë„ ì •ì˜
    progress_updated = Signal(int, str)  # progress%, message
    analysis_completed = Signal(list)    # List[KeywordBasicData] - AI ë¶„ì„ ê²°ê³¼
    analysis_data_updated = Signal(dict) # ì‹¤ì‹œê°„ ë¶„ì„ ë°ì´í„° ì—…ë°ì´íŠ¸
    error_occurred = Signal(str)         # error_message
    
    def __init__(self, product_names: List[str], prompt: str, selected_keywords: List[str] = None, selected_category: str = ""):
        super().__init__()
        self.product_names = product_names
        self.prompt = prompt
        self.selected_keywords = selected_keywords or []  # 1ë‹¨ê³„ì—ì„œ ì„ íƒí•œ í‚¤ì›Œë“œë“¤
        self.selected_category = selected_category  # 1ë‹¨ê³„ì—ì„œ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬
        self._stop_requested = False
        self._mutex = QMutex()
    
    def request_stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­"""
        with QMutexLocker(self._mutex):
            self._stop_requested = True
            
    def stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ (í•˜ìœ„ í˜¸í™˜)"""
        self.request_stop()
    
    def is_stopped(self) -> bool:
        """ì¤‘ë‹¨ ìš”ì²­ í™•ì¸"""
        with QMutexLocker(self._mutex):
            return self._stop_requested
    
    def run(self):
        """ì›Œì»¤ ì‹¤í–‰"""
        try:
            logger.info(f"AI ë¶„ì„ ì‹œì‘")
            
            # 1ë‹¨ê³„: AI API í˜¸ì¶œ
            self.progress_updated.emit(10, "AI ëª¨ë¸ì— ë¶„ì„ ìš”ì²­ ì¤‘...")
            
            if self.is_stopped():
                return
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒí’ˆëª… + ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê²°í•©)
            from .engine_local import build_ai_prompt
            
            # ìƒí’ˆëª…ì—ì„œ title ì¶”ì¶œ
            product_titles = []
            for product in self.product_names:
                if isinstance(product, dict):
                    product_titles.append(product.get('title', ''))
                elif isinstance(product, str):
                    product_titles.append(product)
            
            final_prompt = build_ai_prompt(product_titles, self.prompt)
            
            # ì„¤ì •ëœ AI API í˜¸ì¶œ
            ai_response = self.call_ai_api(final_prompt)
            
            # AI ì‘ë‹µ ë°ì´í„° ì—…ë°ì´íŠ¸
            self.analysis_data_updated.emit({
                'input_prompt': final_prompt,
                'ai_response': ai_response
            })
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(50, "AI ì‘ë‹µ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            
            # 2ë‹¨ê³„: AI ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            from .engine_local import parse_ai_keywords_response, deduplicate_keywords
            
            logger.info(f"ğŸ¤– AI ì‘ë‹µ ê¸¸ì´: {len(ai_response)} ë¬¸ì")
            logger.info(f"ğŸ¤– AI ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {ai_response[:200]}...")
            
            extracted_keywords = parse_ai_keywords_response(ai_response)
            
            logger.info(f"ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: {len(extracted_keywords)}")
            logger.info(f"ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°: {extracted_keywords[:10]}")
            
            if not extracted_keywords:
                self.error_occurred.emit("AIì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return
            
            self.progress_updated.emit(60, f"í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° 1ë‹¨ê³„ í‚¤ì›Œë“œ ë³‘í•© ì¤‘... ({len(extracted_keywords)}ê°œ)")
            
            # 3ë‹¨ê³„: AI ì¶”ì¶œ í‚¤ì›Œë“œì™€ 1ë‹¨ê³„ ì„ íƒ í‚¤ì›Œë“œ ë³‘í•©
            all_keywords = extracted_keywords.copy()  # AI ì¶”ì¶œ í‚¤ì›Œë“œ
            
            # 1ë‹¨ê³„ì—ì„œ ì„ íƒí•œ í‚¤ì›Œë“œ ì¶”ê°€ (ì¤‘ë³µ í™•ì¸)
            if self.selected_keywords:
                logger.info(f"ğŸ“‹ 1ë‹¨ê³„ ì„ íƒ í‚¤ì›Œë“œ {len(self.selected_keywords)}ê°œë¥¼ ë³‘í•©í•©ë‹ˆë‹¤: {self.selected_keywords}")
                all_keywords.extend(self.selected_keywords)
            
            # ì¤‘ë³µ ì œê±° ("ê°•ì•„ì§€ê°„ì‹" = "ê°•ì•„ì§€ ê°„ì‹")
            unique_keywords = deduplicate_keywords(all_keywords)
            
            # AI í‚¤ì›Œë“œì™€ 1ë‹¨ê³„ í‚¤ì›Œë“œ ë³‘í•© ê²°ê³¼ ë¡œê·¸
            ai_count = len(extracted_keywords)
            selected_count = len(self.selected_keywords) if self.selected_keywords else 0
            merged_count = len(unique_keywords)
            removed_duplicates = len(all_keywords) - merged_count
            
            logger.info(f"ğŸ”€ í‚¤ì›Œë“œ ë³‘í•© ì™„ë£Œ: AI {ai_count}ê°œ + 1ë‹¨ê³„ {selected_count}ê°œ = ì´ {merged_count}ê°œ (ì¤‘ë³µ ì œê±°: {removed_duplicates}ê°œ)")
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(70, f"1ë‹¨ê³„: {len(unique_keywords)}ê°œ í‚¤ì›Œë“œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì¤‘... (AI {ai_count}ê°œ + ì„ íƒ {selected_count}ê°œ)")
            
            # 4ë‹¨ê³„: ì›”ê²€ìƒ‰ëŸ‰ë§Œ ë¨¼ì € ì¡°íšŒ (vendors ì§ì ‘ í˜¸ì¶œ)
            logger.info(f"ğŸ“Š 1ë‹¨ê³„ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒí•  í‚¤ì›Œë“œë“¤: {unique_keywords[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸
            
            # ì›”ê²€ìƒ‰ëŸ‰ ë³‘ë ¬ ì¡°íšŒ
            from .adapters import analyze_keywords_with_volume_and_category
            
            # ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì§„í–‰ë¥  ì½œë°± ì •ì˜
            def volume_progress_callback(current: int, total: int, message: str):
                if self.is_stopped():
                    return
                    
                # 70% ~ 80% êµ¬ê°„ì—ì„œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì§„í–‰ë¥  í‘œì‹œ
                progress = 70 + int((current / total) * 10)
                self.progress_updated.emit(progress, f"1ë‹¨ê³„ {current}/{total}: {message}")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ (ì „ì²´ìƒí’ˆìˆ˜ëŠ” 0ìœ¼ë¡œ ì„¤ì •ë¨)
            volume_analyzed = analyze_keywords_with_volume_and_category(
                keywords=unique_keywords,
                max_workers=3,  # 3ê°œì”© ë³‘ë ¬ ì²˜ë¦¬
                stop_check=self.is_stopped,
                progress_callback=volume_progress_callback
            )
            
            # ê²°ê³¼ ì •ë ¬ (ì›ë˜ ìˆœì„œ ìœ ì§€)
            keyword_order = {kw: i for i, kw in enumerate(unique_keywords)}
            volume_analyzed.sort(key=lambda x: keyword_order.get(x.keyword, 999999))
            
            logger.info(f"ğŸ“Š 1ë‹¨ê³„ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì™„ë£Œ: {len(volume_analyzed)}ê°œ ì¤‘ ê²€ìƒ‰ëŸ‰ ìˆëŠ” í‚¤ì›Œë“œ {len([kw for kw in volume_analyzed if kw.search_volume > 0])}ê°œ")
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(80, "ì›”ê²€ìƒ‰ëŸ‰ 100 ì´ìƒ í‚¤ì›Œë“œ í•„í„°ë§ ì¤‘...")
            
            # 5ë‹¨ê³„: ì›”ê²€ìƒ‰ëŸ‰ 100 ì´ìƒ í•„í„°ë§
            from .engine_local import filter_keywords_by_search_volume
            volume_filtered = filter_keywords_by_search_volume(volume_analyzed, 100)
            
            # ë¶„ì„ ë°ì´í„° ì—…ë°ì´íŠ¸ (1ë‹¨ê³„ ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ê²°ê³¼ ë° í•„í„°ë§ ê²°ê³¼)
            self.analysis_data_updated.emit({
                'volume_analyzed': volume_analyzed,
                'volume_filtered': volume_filtered,
                'extracted_keywords': unique_keywords
            })
            
            logger.info(f"ğŸ“Š ì›”ê²€ìƒ‰ëŸ‰ í•„í„°ë§ ì „: {len(volume_analyzed)}ê°œ í‚¤ì›Œë“œ")
            
            if not volume_filtered:
                logger.warning("âš ï¸ ì›”ê²€ìƒ‰ëŸ‰ 100 ì´ìƒì¸ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë§ ê¸°ì¤€ì„ 10ìœ¼ë¡œ ë‚®ì¶¥ë‹ˆë‹¤.")
                # ì›”ê²€ìƒ‰ëŸ‰ ê¸°ì¤€ì„ 10ìœ¼ë¡œ ë‚®ì¶°ì„œ ì¬ì‹œë„
                volume_filtered = filter_keywords_by_search_volume(volume_analyzed, 10)
                
                if not volume_filtered:
                    # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ëª¨ë“  í‚¤ì›Œë“œ ì‚¬ìš©
                    volume_filtered = volume_analyzed
                    logger.info(f"ğŸ“Š ëª¨ë“  í‚¤ì›Œë“œ ì‚¬ìš©: {len(volume_filtered)}ê°œ")
                else:
                    logger.info(f"ğŸ“Š ì›”ê²€ìƒ‰ëŸ‰ 10 ì´ìƒ í•„í„°ë§ ì™„ë£Œ: {len(volume_filtered)}ê°œ í‚¤ì›Œë“œ")
            else:
                logger.info(f"ğŸ“Š ì›”ê²€ìƒ‰ëŸ‰ 100 ì´ìƒ í•„í„°ë§ ì™„ë£Œ: {len(volume_filtered)}ê°œ í‚¤ì›Œë“œ")
            
            # í•„í„°ë§ëœ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°
            for i, kw in enumerate(volume_filtered[:3]):
                logger.info(f"  í•„í„°ë§ëœ í‚¤ì›Œë“œ {i+1}: '{kw.keyword}' (ê²€ìƒ‰ëŸ‰: {kw.search_volume})")
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(85, f"2ë‹¨ê³„: {len(volume_filtered)}ê°œ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì¤‘...")
            
            logger.info(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œì‘: {len(volume_filtered)}ê°œ í‚¤ì›Œë“œ")
            
            # 6ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¡°íšŒ (100 ì´ìƒ í‚¤ì›Œë“œë§Œ)
            from .adapters import analyze_keywords_with_category_only
            
            # ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì§„í–‰ë¥  ì½œë°± ì •ì˜
            def category_progress_callback(current: int, total: int, message: str):
                if self.is_stopped():
                    return
                    
                # 85% ~ 95% êµ¬ê°„ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì§„í–‰ë¥  í‘œì‹œ
                progress = 85 + int((current / total) * 10)
                self.progress_updated.emit(progress, f"2ë‹¨ê³„ {current}/{total}: {message}")
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ì¡°íšŒ ì‹¤í–‰
            try:
                final_keywords = analyze_keywords_with_category_only(
                    keyword_data_list=volume_filtered,
                    max_workers=3,  # 3ê°œì”© ë³‘ë ¬ ì²˜ë¦¬
                    stop_check=self.is_stopped,
                    progress_callback=category_progress_callback
                )
                
                logger.info(f"ğŸ“Š 2ë‹¨ê³„ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì™„ë£Œ: {len(final_keywords)}ê°œ í‚¤ì›Œë“œ")
            except Exception as e:
                logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                # ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì›”ê²€ìƒ‰ëŸ‰ë§Œ ìˆëŠ” ë°ì´í„° ì‚¬ìš©
                final_keywords = volume_filtered
                logger.info(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ ì—†ì´ ì§„í–‰: {len(final_keywords)}ê°œ í‚¤ì›Œë“œ")
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(96, "ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì¤‘...")
            
            # 7ë‹¨ê³„: 1ë‹¨ê³„ì—ì„œ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ì™€ ë§¤ì¹­ë˜ëŠ” í‚¤ì›Œë“œë§Œ í•„í„°ë§
            from .engine_local import filter_keywords_by_category
            
            logger.info(f"ğŸ” ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì‹œì‘: ì„ íƒëœ ì¹´í…Œê³ ë¦¬='{self.selected_category}', ì „ì²´ í‚¤ì›Œë“œ={len(final_keywords)}ê°œ")
            
            # ì „ì²´ í‚¤ì›Œë“œë“¤ì˜ ì¹´í…Œê³ ë¦¬ ë¯¸ë¦¬ë³´ê¸° (ë””ë²„ê¹…ìš©)
            logger.info("ğŸ“Š ì „ì²´ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ëª©ë¡:")
            for i, kw in enumerate(final_keywords[:5]):
                logger.info(f"  í‚¤ì›Œë“œ {i+1}: '{kw.keyword}' â†’ ì¹´í…Œê³ ë¦¬: '{kw.category}'")
            
            if self.selected_category and self.selected_category.strip():
                logger.info(f"ğŸ¯ í•„í„°ë§í•  ì¹´í…Œê³ ë¦¬: '{self.selected_category}'")
                category_matched_keywords = filter_keywords_by_category(final_keywords, self.selected_category)
                logger.info(f"ğŸ“‹ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì™„ë£Œ: ì„ íƒ ì¹´í…Œê³ ë¦¬ '{self.selected_category}'ì™€ ë§¤ì¹­ë˜ëŠ” {len(category_matched_keywords)}ê°œ í‚¤ì›Œë“œ")
                
                # ë””ë²„ê¹…ìš©: ë§¤ì¹­ëœ í‚¤ì›Œë“œë“¤ì˜ ì¹´í…Œê³ ë¦¬ ë¡œê·¸
                for i, kw in enumerate(category_matched_keywords[:3]):
                    logger.info(f"  ë§¤ì¹­ í‚¤ì›Œë“œ {i+1}: '{kw.keyword}' - '{kw.category}'")
                
                if not category_matched_keywords:
                    logger.warning(f"âš ï¸ ì„ íƒ ì¹´í…Œê³ ë¦¬ '{self.selected_category}'ì™€ ë§¤ì¹­ë˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€ (í‚¤ì›Œë“œë¥¼ í‘œì‹œí•˜ì§€ ì•ŠìŒ)
            else:
                # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ëª¨ë“  í‚¤ì›Œë“œ í‘œì‹œ
                category_matched_keywords = final_keywords
                logger.info("ğŸ“‹ ì„ íƒëœ ì¹´í…Œê³ ë¦¬ê°€ ì—†ì–´ ëª¨ë“  í‚¤ì›Œë“œë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
            
            # ìµœì¢… ë¶„ì„ ë°ì´í„° ì—…ë°ì´íŠ¸
            self.analysis_data_updated.emit({
                'final_keywords': final_keywords,
                'category_matched_keywords': category_matched_keywords,
                'selected_category': self.selected_category
            })
            
            self.progress_updated.emit(100, f"AI ë¶„ì„ ì™„ë£Œ: {len(category_matched_keywords)}ê°œ í‚¤ì›Œë“œ (ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì™„ë£Œ)")
            
            # ì™„ë£Œ ì‹œê·¸ë„ ë°œì†¡ (ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ëœ í‚¤ì›Œë“œë“¤)
            self.analysis_completed.emit(category_matched_keywords)
            
            logger.info(f"AI ë¶„ì„ ì™„ë£Œ - ì „ì²´: {len(final_keywords)}ê°œ, í•„í„°ë§: {len(category_matched_keywords)}ê°œ")
            
        except Exception as e:
            logger.error(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
            self.error_occurred.emit(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    def call_ai_api(self, prompt: str) -> str:
        """ì‚¬ìš©ìê°€ ì„¤ì •í•œ AI API í˜¸ì¶œ"""
        try:
            from src.foundation.config import config_manager
            api_config = config_manager.load_api_config()
            
            # í˜„ì¬ ì„ íƒëœ AI ëª¨ë¸ í™•ì¸
            current_model = getattr(api_config, 'current_ai_model', '')
            if not current_model or current_model == "AI ì œê³µìë¥¼ ì„ íƒí•˜ì„¸ìš”":
                raise Exception("AI ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì • ë©”ë‰´ì—ì„œ AI ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ API í˜¸ì¶œ
            if "GPT" in current_model:
                if not hasattr(api_config, 'openai_api_key') or not api_config.openai_api_key:
                    raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"{current_model}ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
                return self.call_openai_direct(prompt, api_config.openai_api_key, current_model)
                
            elif "Gemini" in current_model:
                if not hasattr(api_config, 'gemini_api_key') or not api_config.gemini_api_key:
                    raise Exception("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"{current_model}ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
                return self.call_gemini_direct(prompt, api_config.gemini_api_key, current_model)
                
            elif "Claude" in current_model:
                if not hasattr(api_config, 'claude_api_key') or not api_config.claude_api_key:
                    raise Exception("Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.info(f"{current_model}ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
                return self.call_claude_direct(prompt, api_config.claude_api_key, current_model)
            else:
                raise Exception(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” AI ëª¨ë¸ì…ë‹ˆë‹¤: {current_model}")
                
        except Exception as e:
            logger.error(f"AI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise Exception(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def call_openai_direct(self, prompt: str, api_key: str, model_name: str) -> str:
        """OpenAI API ì§ì ‘ í˜¸ì¶œ"""
        import requests
        
        try:
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            
            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì‹¤ì œ ëª¨ë¸ ID ì„¤ì •
            if "GPT-4o Mini" in model_name:
                model_id = "gpt-4o-mini"
                max_tokens = 16384
            elif "GPT-4o" in model_name and "Mini" not in model_name:
                model_id = "gpt-4o"
                max_tokens = 8192
            elif "GPT-4 Turbo" in model_name:
                model_id = "gpt-4-turbo"
                max_tokens = 8192
            else:
                model_id = "gpt-4o-mini"  # ê¸°ë³¸ê°’
                max_tokens = 16384
            
            payload = {
                "model": model_id,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.3
            }
            
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'choices' in data and len(data['choices']) > 0:
                    return data['choices'][0]['message']['content']
                else:
                    raise Exception("OpenAI API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise Exception(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def call_gemini_direct(self, prompt: str, api_key: str, model_name: str) -> str:
        """Gemini API ì§ì ‘ í˜¸ì¶œ"""
        import requests
        
        try:
            headers = {
                'Content-Type': 'application/json'
            }
            
            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì‹¤ì œ ëª¨ë¸ ID ì„¤ì •
            if "Gemini 1.5 Flash" in model_name:
                model_id = "gemini-1.5-flash-latest"
                max_tokens = 8192
            elif "Gemini 1.5 Pro" in model_name:
                model_id = "gemini-1.5-pro-latest"
                max_tokens = 8192
            elif "Gemini 2.0 Flash" in model_name:
                model_id = "gemini-2.0-flash-exp"
                max_tokens = 8192
            else:
                model_id = "gemini-1.5-flash-latest"  # ê¸°ë³¸ê°’
                max_tokens = 8192
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "temperature": 0.3,
                    "maxOutputTokens": max_tokens
                }
            }
            
            url = f'https://generativelanguage.googleapis.com/v1beta/models/{model_id}:generateContent?key={api_key}'
            
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'candidates' in data and len(data['candidates']) > 0:
                    content = data['candidates'][0].get('content', {})
                    parts = content.get('parts', [])
                    if parts:
                        return parts[0].get('text', '')
                    else:
                        raise Exception("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                else:
                    raise Exception("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"Gemini API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def call_claude_direct(self, prompt: str, api_key: str, model_name: str) -> str:
        """Claude API ì§ì ‘ í˜¸ì¶œ"""
        import requests
        
        try:
            headers = {
                'x-api-key': api_key,
                'Content-Type': 'application/json',
                'anthropic-version': '2023-06-01'
            }
            
            # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì‹¤ì œ ëª¨ë¸ ID ì„¤ì •
            if "Claude 3.5 Sonnet" in model_name:
                model_id = "claude-3-5-sonnet-20241022"
                max_tokens = 8192
            elif "Claude 3.5 Haiku" in model_name:
                model_id = "claude-3-5-haiku-20241022"
                max_tokens = 8192
            elif "Claude 3 Opus" in model_name:
                model_id = "claude-3-opus-20240229"
                max_tokens = 8192
            else:
                model_id = "claude-3-5-sonnet-20241022"  # ê¸°ë³¸ê°’
                max_tokens = 8192
            
            payload = {
                "model": model_id,
                "max_tokens": max_tokens,
                "temperature": 0.3,
                "messages": [{"role": "user", "content": prompt}]
            }
            
            response = requests.post(
                'https://api.anthropic.com/v1/messages',
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                if 'content' in data and len(data['content']) > 0:
                    return data['content'][0]['text']
                else:
                    raise Exception("Claude API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"Claude API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise Exception(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")


class ProductNameGenerationWorker(QThread):
    """4ë‹¨ê³„: AI ìƒí’ˆëª… ìƒì„± ì „ìš© ì›Œì»¤ (ì›”ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì—†ìŒ)"""
    
    # ì‹œê·¸ë„ ì •ì˜
    progress_updated = Signal(int, str)  # progress%, message
    generation_completed = Signal(str)   # ìƒì„±ëœ ìƒí’ˆëª…ë“¤ í…ìŠ¤íŠ¸
    error_occurred = Signal(str)         # error_message
    
    def __init__(self, prompt: str):
        super().__init__()
        self.prompt = prompt
        self._stop_requested = False
        self._mutex = QMutex()
    
    def request_stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­"""
        with QMutexLocker(self._mutex):
            self._stop_requested = True
            
    def stop(self):
        """ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ (í•˜ìœ„ í˜¸í™˜)"""
        self.request_stop()
    
    def is_stopped(self) -> bool:
        """ì¤‘ë‹¨ ìš”ì²­ í™•ì¸"""
        with QMutexLocker(self._mutex):
            return self._stop_requested
    
    def run(self):
        """ì›Œì»¤ ì‹¤í–‰ - AI ìƒí’ˆëª… ìƒì„±ë§Œ ìˆ˜í–‰"""
        try:
            logger.info("AI ìƒí’ˆëª… ìƒì„± ì‹œì‘")
            
            self.progress_updated.emit(20, "AI ëª¨ë¸ì— ìƒí’ˆëª… ìƒì„± ìš”ì²­ ì¤‘...")
            
            if self.is_stopped():
                return
            
            # AI API ì§ì ‘ í˜¸ì¶œ (í”„ë¡¬í”„íŠ¸ëŠ” ì´ë¯¸ ì™„ì„±ëœ ìƒíƒœ)
            ai_response = self.call_ai_api(self.prompt)
            
            if self.is_stopped():
                return
            
            self.progress_updated.emit(80, "AI ì‘ë‹µ ì²˜ë¦¬ ì¤‘...")
            
            if not ai_response or not ai_response.strip():
                self.error_occurred.emit("AIì—ì„œ ìƒí’ˆëª…ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return
            
            self.progress_updated.emit(100, "ìƒí’ˆëª… ìƒì„± ì™„ë£Œ!")
            
            # ìƒì„±ëœ ìƒí’ˆëª… ê²°ê³¼ ì „ë‹¬
            self.generation_completed.emit(ai_response.strip())
            
            logger.info("AI ìƒí’ˆëª… ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"AI ìƒí’ˆëª… ìƒì„± ì‹¤íŒ¨: {e}")
            self.error_occurred.emit(f"AI ìƒí’ˆëª… ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def call_ai_api(self, prompt: str) -> str:
        """AI API í˜¸ì¶œ - AIAnalysisWorkerì˜ ë©”ì„œë“œ ì¬ì‚¬ìš©"""
        # ì„ì‹œ AIAnalysisWorker ì¸ìŠ¤í„´ìŠ¤ ìƒì„±í•˜ì—¬ API í˜¸ì¶œ ë©”ì„œë“œ ì¬ì‚¬ìš©
        temp_worker = AIAnalysisWorker([], prompt)
        return temp_worker.call_ai_api(prompt)


# ì „ì—­ ì›Œì»¤ ë§¤ë‹ˆì €
worker_manager = WorkerManager()